
# coding: utf-8

# In[ ]:


import numpy as np
import deep_laa_support as dls
import random
import sys
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
from sklearn.cluster import KMeans
from moco import moco_training_step
from moco import MoCoQueue, update_model_via_ema
import tensorflow.keras.backend as K
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt


import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

EMBEDDING_DIM = 40
# queue = MoCoQueue(EMBEDDING_DIM, 256)       #256æŒ‡é˜Ÿåˆ—é•¿åº¦ï¼Œä»embeddingä¸­å–ä¸¤ä¸ªä½œä¸ºkey   é˜Ÿåˆ—æ›´æ–°

# model_lb = tf.keras.Sequential(layers=[tf.keras.layers.Dense(256, activation='relu'),tf.keras.layers.Dense(40, activation=None)])
# model_ema_lb = tf.keras.Sequential(layers=[tf.keras.layers.Dense(256, activation='relu'),tf.keras.layers.Dense(40, activation=None)])
# model_ema_lb = tf.keras.Sequential(
#     [
#         tf.keras.layers.InputLayer(input_shape=(287, )),
#         tf.keras.layers.Dense(256, activation='relu'),
#         tf.keras.layers.Dense(40, activation=None),
#     ]
# )
#
# model = tf.keras.Sequential(
#     [
#         tf.keras.layers.InputLayer(input_shape=(287, )),
#         tf.keras.layers.Dense(256, activation='relu'),
#         tf.keras.layers.Dense(40, activation=None),
#     ]
# )
#
# model_ema = tf.keras.Sequential(
#     [
#         tf.keras.layers.InputLayer(input_shape=(287, )),
#         tf.keras.layers.Dense(256, activation='relu'),
#         tf.keras.layers.Dense(40, activation=None),
#     ]
# )
# class MyModel(tf.keras.Model):
#     def __init__(self):
#         super(MyModel, self).__init__()
#        # self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu')    #å·ç§¯,åº”è¯¥å¯ä»¥ä¸ç”¨
#         #self.flatten = tf.keras.layers.Flatten()                         #å±•å¼€,åº”è¯¥ä¹Ÿä¸éœ€è¦
#         # self.dense1 = self.add_weight(name='weight_vector1', shape=(287, 256),
#         #                               initializer='glorot_uniform', trainable=True,
#         #                               regularizer=None)
#         # self.dense2 = self.add_weight(name='weight_vector2', shape=(256, 40),
#         #                               initializer='glorot_uniform', trainable=True,
#         #                               regularizer=None)
#         self.d1 = tf.keras.layers.Dense(256, activation='relu')
#         self.d2 = tf.keras.layers.Dense(EMBEDDING_DIM, activation=None) #åªéœ€è¦åé¢çš„ä¸¤å±‚
#         #self.d3 = dls.moco_full_connect_relu_BN(moco_x, [feature_size, moco_h1_size_encoder])
#
#     def call(self, x):
#      #   x = self.conv1(x)                                       #åˆ é™¤
#        # x = self.flatten(x)                                     #åˆ é™¤
#         x = self.d1(x)
#         self.y = self.d2(x)
#         return self.y
#
#
# # Create an instance of the model
# model = MyModel()
# model_ema = MyModel()
# print("*************************")
# print(model_ema.layers[0])
# print("lb************")


# read data
# filename = 'default_file'
filename = 'feature_48_L'
data_all = np.load(filename +'.npz')
print('File ' + filename + '.npz ' 'loaded.')
user_labels = data_all['user_labels']         #æ ‡ç­¾é›†
true_labels = data_all['true_labels']
category_size = data_all['category_num']
print(category_size)
source_num = data_all['source_num']
print(source_num)
feature = data_all['feature']               #ç‰¹å¾é›†
_, feature_size = np.shape(feature)
n_samples, _ = np.shape(true_labels)

def left_NN(input, training=None):
    flatten_input = tf.layers.flatten(input)
    print(flatten_input)
    flatten_input = tf.keras.layers.BatchNormalization(center=False, scale=False)(flatten_input)
    print(flatten_input)
    flatten_input =tf.layers.dense(inputs= flatten_input,units=128, activation='relu')
    x = tf.layers.dropout(flatten_input, training=None)
    print(x)
    x = tf.keras.layers.BatchNormalization(center=False, scale=False)(x)
    cls_out = tf.nn.softmax(tf.layers.dense(inputs=x,units=2), axis=-1)
    return cls_out
# with tf.GradientTape() as tape:
#     feature_lb = left_NN(feature)
x_train = feature
# print(feature_lb)

print("***************")
# print(x_train.dtype)
x_train = x_train.astype(np.float32)
#x_train = x_train.batch(6033)


answers_bin_missings = []
for i in range(len(user_labels)):       #6033
    row = []
    for r in range(source_num):         #498
        row1 = []
        k1=2*r
        k2 = 2*r + 1
        row1.append(user_labels[i][k1])
        row1.append(user_labels[i][k2])
        row.append(row1)
    answers_bin_missings.append(row)
answers_bin_missings = np.array(answers_bin_missings, dtype=np.float32)

# Initialise the models and make the EMA model 90% similar to the main model  æ»‘åŠ¨æ¨¡å‹ ,mocoæ˜¯ä»¥æ»‘åŠ¨çª—å£æ¥è¿›è¡Œçš„
# model(x_train[:1])              #å–ä¸€ä¸ª
# model_ema(x_train[:1])
# update_model_via_ema(model, model_ema, 0.1)

# loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
#
# optimizer = tf.keras.optimizers.Adam()

#================= basic parameters =====================
# define batch size (use all samples in one batch)
batch_size = n_samples # n_samples ä¾‹å­ä¸ªæ•° ç›¸å½“äºä¸€ä¸ªbatch
moco_batch_size = 128
cluster_num = 160
T = 1 # mc_sampling_times  é‡‡æ ·æ¬¡æ•°
p_pure = np.array([0.5, 0.5], dtype=np.float32)

if np.max(feature) <= 1 and np.min(feature) >= 0:
    flag_node_type = 'Bernoulli'
else:
    flag_node_type = 'Gaussian'   
print(flag_node_type + ' output nodes are used.')

# æ·»åŠ Mocoéƒ¨åˆ†#
# with tf.name_scope('moco'):
#     moco_x = tf.placeholder(dtype=tf.float32, shape=[moco_batch_size, feature_size], name='x_moco_input')  # (128,287)  éšæœºæŠ½å–128ä¸ªä½œä¸ºä¸€ä¸ªbatch,åŒæ‰€æœ‰çš„è¿›è¡Œæ¯”è¾ƒ  è¿™ä¸ªxçš„æ¥æºå¯èƒ½éœ€è¦ä¸€ä¸ªå°çš„VAEç½‘ç»œ
#
#     moco_h1_size_encoder = int(np.floor(feature_size / 2.0))  # 287/2
#     encoder_q = dls.moco_full_connect_relu_BN(moco_x, [feature_size, moco_h1_size_encoder])
#     encoder_q = dls.moco_full_connect(encoder_q, [moco_h1_size_encoder, 40])
#     encoder_q = tf.math.l2_normalize(encoder_q, axis = -1)
#     #momentum encoder
#     encoder_k = dls.moco_full_connect_relu_BN(moco_x, [feature_size, moco_h1_size_encoder])
#     encoder_k = dls.moco_full_connect(encoder_k, [moco_h1_size_encoder, 40])
#     encoder_k = tf.math.l2_normalize(encoder_k, axis = -1)
#
####æ·»åŠ å¯¹æ¯”å­¦ä¹ éƒ¨åˆ†,ä¸ä½¿ç”¨Keras#########
#
# with tf.name_scope('moco_cla'):
#     moco_h1_size_encoder = int(np.floor(feature_size/2.0))  #287/2
#     moco_h2_size_encoder = 100
#     moco_embedding_size = 40
#     moco_h1_size_decoder = 100
#     moco_h2_size_decoder = int(np.floor(feature_size/2.0))
#     with tf.variable_scope('moco_encoder'):
#         x = tf.placeholder(dtype=tf.float32, shape=[batch_size, feature_size], name='x_input')  #(6033,287)
#         # mu_hx[batch_size, embedding_size]
#         # sigma_hx[batch_size, embedding_size]
#         with tf.variable_scope('feature_encoder_h1'):
#             _h1_encoder, w1_encoder, b1_encoder = dls.full_connect_relu_BN(x, [feature_size, moco_h1_size_encoder])  #tf.nn.relu(_tmp_results), weights, biases  æ­¤å¤„ä½¿ç”¨äº†æ­£æ€åˆ†å¸ƒ  w,x,b
#         with tf.variable_scope('feature_encoder_h2'):
#             _h2_encoder, w2_encoder, b2_encoder = dls.full_connect_relu_BN(_h1_encoder, [moco_h1_size_encoder, moco_h2_size_encoder])
#         with tf.variable_scope('feature_encoder_mu'):
#             mu_hx, w_mu_encoder, b_mu_encoder = dls.full_connect(_h2_encoder, [moco_h2_size_encoder, moco_embedding_size])            #æ­£æ€åˆ†å¸ƒä¸­çš„ğœ‡ 40
#         with tf.variable_scope('feature_encoder_sigma'):
#             sigma_hx, w_sigma_encoder, b_sigma_encoder = dls.full_connect(_h2_encoder, [moco_h2_size_encoder, moco_embedding_size])   #æ­£æ€åˆ†å¸ƒä¸­çš„ğœ

def make_q(z, batch_size, alpha):
    sqd_dist_mat = pairwise_sqd_distance(z, batch_size)
    print(sqd_dist_mat.shape)
    q = tf.pow((1 + sqd_dist_mat / alpha), -(alpha + 1) / 2)
    print(q.shape)
    print(tf.zeros(shape=[batch_size]))
    q = tf.matrix_set_diag(q, tf.zeros(shape=[batch_size]))
    q = q / tf.reduce_sum(q, axis=0, keepdims=True)
    # q = 0.5*(q + tf.transpose(q))
    q = tf.clip_by_value(q, 1e-10, 1.0)

    return q

def pairwise_sqd_distance(X, batch_size):

    tiled = tf.tile(tf.expand_dims(X, axis=1), tf.stack([1, batch_size, 1]))
    tiled_trans = tf.transpose(tiled, perm=[1,0,2])
    diffs = tiled - tiled_trans
    sqd_dist_mat = tf.reduce_sum(tf.square(diffs), axis=2)
    sqd_dist_mat = tf.Print(sqd_dist_mat,[sqd_dist_mat])

    return sqd_dist_mat

def identity_init(shape):
    out = np.ones(shape, dtype=np.float32) * 0
    if len(shape) == 3:
        for r in range(shape[0]):
            for i in range(shape[1]):
                out[r, i, i] = 2
    elif len(shape) == 2:
        for i in range(shape[1]):
            out[i, i] = 2
    return out


def mig_loss_fuction(left_out, right_out):

    batch_num = left_out.shape[0]
    batch_num1 = tf.cast(batch_num, dtype=tf.float32)  #lb add

    I = tf.cast(np.eye(batch_num), dtype=tf.float32)
    E = tf.cast(np.ones((batch_num, batch_num)), dtype=tf.float32)
    normalize_1 = batch_num
    normalize_2 = batch_num * (batch_num - 1 )
    normalize_1 = tf.cast(normalize_1, dtype=tf.float32) #lb add
    normalize_2 = tf.cast(normalize_2, dtype=tf.float32) #lb add

    new_output = left_out / p_pure
    m = tf.matmul(new_output, right_out, transpose_b=True)
    noise = np.random.rand(1) * 0.0001
    m1 = tf.math.log(m * I + I * noise + E - I) # i<->i + i<->j. æ­¤å¤„E - Iæ˜¯ä¸ºäº†è®©logä¸º0ï¼Œä»¥ä¾¿æœ€å¤§åŒ–P(i,i)äº’ä¿¡æ¯
    # m1 = tf.math.log(m * I + E - I)
    m2 = m * (E - I) # i<->jï¼Œæœ€å°åŒ–P(i,j)äº’ä¿¡æ¯
    # print(m)

    #loss æ¥è‡ª KLï¼Œä¸MIGç›¸åæ•°
    return -(tf.reduce_sum(tf.reduce_sum(m1)) + batch_num1) / normalize_1 + tf.reduce_sum(tf.reduce_sum(m2)) / normalize_2
#================= encoder q(y|l) and q(h|x) =====================
with tf.name_scope('encoder'):
    #================= q(y|l) =====================æ ‡ç­¾é›†
    # define input l (source label vectors) æºæ ‡ç­¾å‘é‡
    input_size = source_num * category_size    #464*2 =user labelå¤§å°
    with tf.variable_scope('q_yl'):
        l = tf.placeholder(dtype=tf.float32, shape=[batch_size, input_size], name='l_input')   #6033,928
        pi_yl, weights_yl, biases_yl = dls.LAA_encoder(l, batch_size, source_num, category_size) #1,6033,2   æƒé‡,åå·®w,b
    # loss: cross entropy between y_classifier and y_target for pre-training classifier
    with tf.variable_scope('q_yl'):
        pi_yl_target = tf.placeholder(dtype=tf.float32, shape=[batch_size, category_size], name='pi_yl_target')  #6033,2
        loss_yl = dls.LAA_loss_classifier(pi_yl, pi_yl_target)     #äº¤å‰ç†µ
    # optimizier
    learning_rate_yl = 0.01
    optimizer_pre_train_yl = tf.train.AdamOptimizer(learning_rate=learning_rate_yl).minimize(loss_yl)

    #================= q(h|x) =====================ç‰¹å¾é›†  å¤šå…ƒæ­£æ€åˆ†å¸ƒ
    h1_size_encoder = int(np.floor(feature_size/2.0))  #287/2
    h2_size_encoder = 100
    embedding_size = 40
    h1_size_decoder = 100
    h2_size_decoder = int(np.floor(feature_size/2.0))

    with tf.variable_scope('q_hx'):
        x = tf.placeholder(dtype=tf.float32, shape=[batch_size, feature_size], name='x_input')  #(6033,287)
        # mu_hx[batch_size, embedding_size]
        # sigma_hx[batch_size, embedding_size]
        # with tf.variable_scope('feature_encoder_att'):
        #     _att_encoder, _, _ = dls.full_connect1(x, [feature_size, 128])  #tf.nn.relu(_tmp_results), weights, biases  æ­¤å¤„ä½¿ç”¨äº†æ­£æ€åˆ†å¸ƒ  w,x,b
        # with tf.variable_scope('feature_encoder_att'):
        #     _att1_encoder, _, _ = dls.full_connect1(_att_encoder, [256, 128])  #tf.nn.relu(_tmp_results), weights, biases  æ­¤å¤„ä½¿ç”¨äº†æ­£æ€åˆ†å¸ƒ  w,x,b
        with tf.variable_scope('feature_encoder_h1'):
            _h1_encoder, w1_encoder, b1_encoder = dls.full_connect_relu_BN(x, [feature_size, h1_size_encoder])  #tf.nn.relu(_tmp_results), weights, biases  æ­¤å¤„ä½¿ç”¨äº†æ­£æ€åˆ†å¸ƒ  w,x,b 287,287/2
        with tf.variable_scope('feature_encoder_h2'):
            _h2_encoder, w2_encoder, b2_encoder = dls.full_connect_relu_BN(_h1_encoder, [h1_size_encoder, h2_size_encoder])  #100
        with tf.variable_scope('feature_encoder_mu'):
            mu_hx, w_mu_encoder, b_mu_encoder = dls.full_connect(_h2_encoder, [h2_size_encoder, embedding_size])            #æ­£æ€åˆ†å¸ƒä¸­çš„ğœ‡ 40
        with tf.variable_scope('feature_encoder_sigma'):
            sigma_hx, w_sigma_encoder, b_sigma_encoder = dls.full_connect(_h2_encoder, [h2_size_encoder, embedding_size])   #æ­£æ€åˆ†å¸ƒä¸­çš„ğœ
        # with tf.variable_scope('feature_softmax'):
        #     leftout = left_NN(x_train)
        #     print("lb leftout is",leftout)
            # mu_hx_sofamax, w_mu_encoder_softmax, b_mu_encoder_softmax = dls.full_connect(_h2_encoder, [h2_size_encoder, 64])    #lb:ä½¿ç”¨åˆ†ç±»å™¨è¿›è¡Œåˆ†ç±»,å¾—åˆ°åˆ†ç±»ç»“æœ
        #     mu_hx_dropout=tf.nn.dropout(mu_hx_sofamax,0.5)
        # with tf.variable_scope('feature_softmax_result'):
        #     feature_softmax_result = dls.full_connect_softmax(mu_hx_dropout,[64, 2])
        # mu_hx, sigma_hx = dls.vae_encoder(x, feature_size, h1_size_encoder, h2_size_encoder, embedding_size)
        # embedding_h[batch_size, T, embedding_size]   è¿™éƒ¨åˆ†ç›¸å½“äºæ˜¯é‡å‚æ•°åŒ–æŠ€æœ¯ï¼Œå°†â€œé‡‡æ ·â€è¿‡ç¨‹ä½œä¸ºè¾“å…¥
        embedding_h = tf.reshape(mu_hx, [batch_size, 1, embedding_size])+ tf.reshape(sigma_hx, [batch_size, 1, -1])* tf.random_normal(shape=[batch_size, T, embedding_size], mean=0, stddev=1, dtype=tf.float32) #shape=(6033, 1, 40)
    # with tf.variable_scope('crowd_ann'):  # æ·»åŠ äººç¾¤æ¨æ–­right   #åŸæœ‰è®¾è®¡éœ€è¦æ·»åŠ batchsizeï¼Œè¿™é‡Œæš‚æ—¶ä¸æ·»åŠ  batchsizeè®¾å®šä¸º1
    #     # y_ann = tf.placeholder(dtype=tf.float32, shape=[batch_size, feature_size], name='y_crowd')
    #     # leftout = left_NN(x_train)
    #     kernel = tf.Variable(identity_init((498, 2, 2)))
    #     crowd_answer = tf.transpose(answers_bin_missings, (1, 0, 2))   #batch_answers_bin_missings
    #     crowd_emb = tf.matmul(crowd_answer, kernel)
    #     agg_emb = tf.reduce_sum(crowd_emb, axis=0)
    #     type = 2
    #     out = 0
    #     if type == 1:
    #         print(agg_emb.shape)
    #         # print(feature_softmax_result.shape)
    #         print(p_pure.shape)
    #         out = agg_emb + tf.math.log(leftout + 0.001) + tf.math.log(p_pure)
    #     elif type == 2:
    #         out = agg_emb + tf.math.log(p_pure)
    #     elif type == 3:
    #         out = agg_emb + tf.math.log(leftout + 0.001)
    #     with tf.variable_scope('crowd_encoder_h1'):
    #         rightout = tf.nn.softmax(out,axis=-1)
    #     learning_rate = 1e-4
    #     # optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    #     loss_migmax = mig_loss_fuction(leftout, rightout)
    #     # vars = tf.GradientTape().watched_variables()
    #     # grads = tf.GradientTape().gradient(loss, vars)
    #     # grads = tf.gradients(loss, vars)   #lb add
    #     # optimizer.apply_gradients(zip(grads, vars))
    #     # tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_migmax)
    #     print("lb normal")
    with tf.variable_scope('q_hx_AE'):
        # x_reconstr, _, _ = dls.vae_decoder(mu_hx, embedding_size, h1_size_decoder, h2_size_decoder, feature_size)
        # with tf.variable_scope('feature_decoder_att1'):
        #     mu_hx, w1_decoder_att1, w1_decoder_att2 = dls.full_connect_relu_BN(mu_hx, [embedding_size, h1_size_decoder])
        # with tf.variable_scope('feature_decoder_att2'):
        #     mu_hx, _, _ = dls.full_connect1(mu_hx, [h1_size_decoder, embedding_size])
        with tf.variable_scope('feature_decoder_h1'):
            _h1_decoder, w1_decoder, b1_decoder = dls.full_connect_relu_BN(mu_hx, [embedding_size, h1_size_decoder])
        with tf.variable_scope('feature_decoder_h2'):
            _h2_decoder, w2_decoder, b2_decoder = dls.full_connect_relu_BN(_h1_decoder, [h1_size_decoder, h2_size_decoder])
        with tf.variable_scope('feature_decoder_rho'):
            if flag_node_type == 'Bernoulli':
                x_reconstr, w_rho_decoder, b_rho_decoder = dls.full_connect_sigmoid(_h2_decoder, [h2_size_decoder, feature_size])
            elif flag_node_type == 'Gaussian':
                x_reconstr, w_rho_decoder, b_rho_decoder = dls.full_connect(_h2_decoder, [h2_size_decoder, feature_size])
    # ####æ·»åŠ Zç½‘ç»œ####å±‚æ•°åº”è¯¥ä¸º40-287-100-40
    # with tf.variable_scope('space_z'):
    #     with tf.variable_scope('space_z__h1'):
    #         z_h1_decoder, z_w1_decoder, z_b1_decoder = dls.full_connect_relu_BN(mu_hx, [embedding_size, feature_size])
    #     with tf.variable_scope('space_z__h2'):
    #         z_h2_decoder, z_w2_decoder, z_b2_decoder = dls.full_connect_relu_BN(z_h1_decoder,[feature_size, h2_size_decoder])
    #     with tf.variable_scope('space_z_rho'):
    #         if flag_node_type == 'Bernoulli':
    #             z_reconstr, z_w_rho_decoder, z_b_rho_decoder = dls.full_connect_sigmoid(z_h2_decoder,[h2_size_decoder, embedding_size])
    #         elif flag_node_type == 'Gaussian':
    #             z_reconstr, z_w_rho_decoder, z_b_rho_decoder = dls.full_connect(z_h2_decoder,[h2_size_decoder, embedding_size])
    #
    #     reconstr_loss = tf.reduce_mean(tf.squared_difference(x, x_reconstr), axis=1)
    #     batch_size1 = tf.shape(mu_hx)[0]
    #     p = make_q(mu_hx, batch_size1, alpha=20)
    #     q = make_q(z_reconstr, batch_size1, alpha=1)
    #     latent_loss = tf.reduce_sum(-(tf.multiply(p, tf.log(q))))
    #     # Joint loss.å…±åŒæŸå¤±ã€‚
    #     joint_loss = tf.constant(1.0) * reconstr_loss + tf.constant(0.01) * latent_loss
        # Bernoulli åˆ†ç±»åˆ†å¸ƒCat
        loss_cross_entropy_AE = -tf.reduce_mean(tf.reduce_sum(x*tf.log(1e-10+x_reconstr) + (1.0-x)*tf.log(1e-10+(1.0-x_reconstr)), -1))       #äº¤å‰ç†µæŸå¤±  æ¯è¡Œæ±‚å’Œï¼Œå¹³å‡
        # loss_cross_entropy_AE1 = -tf.reduce_mean(tf.reduce_sum(mu_hx * tf.log(1e-10 + z_reconstr) + (1.0 - mu_hx) * tf.log(1e-10 + (1.0 - z_reconstr)),-1))  # äº¤å‰ç†µæŸå¤±  æ¯è¡Œæ±‚å’Œï¼Œå¹³å‡
        # Gaussian æ­£æ€åˆ†å¸ƒ
        loss_square_AE = 0.5 * tf.reduce_mean(tf.square(x_reconstr - x))
        # loss_square_AE1 = 0.5 * tf.reduce_mean(tf.square(z_reconstr - mu_hx))
        constraint_w_AE = 0.5 * (tf.reduce_mean(tf.square(w1_encoder)) + tf.reduce_mean(tf.square(b1_encoder))
            + tf.reduce_mean(tf.square(w2_encoder)) + tf.reduce_mean(tf.square(b2_encoder))
            + tf.reduce_mean(tf.square(w_mu_encoder)) + tf.reduce_mean(tf.square(b_mu_encoder))
            + tf.reduce_mean(tf.square(w1_decoder)) + tf.reduce_mean(tf.square(b1_decoder))
            + tf.reduce_mean(tf.square(w2_decoder)) + tf.reduce_mean(tf.square(b2_decoder))
            + tf.reduce_mean(tf.square(w_rho_decoder)) + tf.reduce_mean(tf.square(b_rho_decoder)))            #KL
        if flag_node_type == 'Bernoulli':
            loss_AE = loss_cross_entropy_AE                 + constraint_w_AE                     #æœ€ç»ˆçš„ ç»¼åˆæŸå¤±=é‡å»ºæŸå¤±loss_square_AE +KLæŸå¤±constraint_w_AE
        elif flag_node_type == 'Gaussian':
            loss_AE = loss_square_AE                 + constraint_w_AE
        learning_rate_AE = 0.02
        optimizer_AE = tf.train.AdamOptimizer(learning_rate=learning_rate_AE).minimize(loss_AE)

    #================= p(x|h) =====================
    with tf.variable_scope('q_hx_AE'):
        with tf.variable_scope('feature_decoder_h1', reuse=True):
            _h_VAE = tf.reshape(embedding_h, [-1, embedding_size])     #embedding_h->q(h|x)
            _h1_decoder_VAE, _, _ = dls.full_connect_relu_BN(_h_VAE, [embedding_size, h1_size_decoder])
        with tf.variable_scope('feature_decoder_h2', reuse=True):
            _h2_decoder_VAE, _, _ = dls.full_connect_relu_BN(_h1_decoder_VAE, [h1_size_decoder, h2_size_decoder])
        with tf.variable_scope('feature_decoder_rho', reuse=True):
            if flag_node_type == 'Bernoulli':
                mu_xh, _, _ = dls.full_connect_sigmoid(_h2_decoder_VAE, [h2_size_decoder, feature_size])        #p(x|h),xä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¼¯åŠªåˆ©åˆ†å¸ƒ å‘é‡ğœŒ_ğ‘¥(h) âˆˆ ã€–["0,1" ]ã€—^ğ¼é€šè¿‡æƒé‡ä¸ºğ‘¤_(ğœŒ_ğ‘¥ )çš„å¤šå±‚ç¥ç»ç½‘ç»œè®¡ç®— å°±æ˜¯ğœ‡
            elif flag_node_type == 'Gaussian':
                mu_xh, _, _ = dls.full_connect(_h2_decoder_VAE, [h2_size_decoder, feature_size])
            mu_xh = tf.reshape(mu_xh, [batch_size, T, feature_size])    #å‘é‡ğœŒ_ğ‘¥(h)
            
    print('Encoders are constructed.')
    
#================= decoder p(l|y), p(x|h), p(y|z), p(h|z) and p(z) =====================
with tf.name_scope('decoder'):
    #================= p(l|y) =====================
    #æ¯ä¸ªå·¥äººnéƒ½æœ‰è‡ªå·±çš„åˆ†å¸ƒå‚æ•° ğœ‹_ğ‘™^ğ‘› (ğ²)ï¼Œå…¶ä¸­yä½œä¸ºè¾“å…¥ï¼Œä½¿ç”¨æƒé‡çŸ©é˜µğ‘¤_(ğœ‹_ğ‘™)^ğ‘›çš„ç¥ç»ç½‘ç»œè®¡ç®—   è®¡ç®—æ¯ä¸ªå·¥äººçš„æ ‡ç­¾
    with tf.variable_scope('p_ly'):
        # pi_ly[category_size, 1, source_num*category_size]     yåœ¨encoderä¸­è¿›è¡Œè®¡ç®—q(y|l)
        pi_ly, weights_ly, biases_ly = dls.LAA_decoder(source_num, category_size)              #å†…åµŒæ­£æ€åˆ†å¸ƒ

        constraint_w_LAA = 0.5 * (tf.reduce_mean(tf.square(weights_ly)) + tf.reduce_mean(tf.square(biases_ly))
            + tf.reduce_mean(tf.square(weights_yl)) + tf.reduce_mean(tf.square(biases_yl)))                     #KL
        
    #================= p(y|z) =====================
    #åœ¨ç»™å®šç°‡ç´¢å¼• z çš„æƒ…å†µä¸‹ç»˜åˆ¶ä¸­é—´æ ‡ç­¾ y  p(y|z)=Cat(ğœ‹yz)
    with tf.variable_scope('p_yz'):
        # pi_yz[cluster_num, category_size]
        _pi_yz = tf.get_variable('pi_yz', dtype=tf.float32,
                                initializer=tf.random_normal(shape=[cluster_num, category_size], mean=0, stddev=1, dtype=tf.float32))  #p(y|z)=Cat(ğœ‹yz)  (200,2)  åˆ›å»ºå˜é‡
        __pi_yz = tf.exp(_pi_yz)
        pi_yz = tf.div(__pi_yz, tf.reduce_sum(__pi_yz, -1, keepdims=True))
        
        pi_yz_assign = tf.placeholder(dtype=tf.float32, shape=[cluster_num, category_size], name='pi_yz_assign')
        initialize_pi_yz = tf.assign(_pi_yz, pi_yz_assign)
        
    #================= p(h|z) =====================
    with tf.variable_scope('p_hz'):
        # mu_hz[cluster_num, embedding_size]    p(h|z)é›†ç¾¤zä¸­ç»˜åˆ¶æ½œåœ¨çš„åµŒå…¥å‘é‡h å¤šå…ƒæ­£æ€åˆ†å¸ƒ
        # sigma_hz[cluster_num, embedding_size]
        mu_hz = tf.get_variable('mu_hz', dtype=tf.float32, initializer=tf.random_normal(shape=[cluster_num, embedding_size], mean=0, stddev=1, dtype=tf.float32))   #å®šä¹‰ğœ‡ å‡å€¼ ï¼ˆ200,40ï¼‰
        sigma_hz = tf.get_variable('sigma_hz', dtype=tf.float32, initializer=tf.ones([cluster_num, embedding_size], dtype=tf.float32))  #å®šä¹‰ ğœ ,æ–¹å·® (200,40)

        mu_hz_assign = tf.placeholder(dtype=tf.float32, shape=[cluster_num, embedding_size], name='mu_hz_assign')    #(200,40)  è¾“å…¥
        initialize_mu_hz = tf.assign(mu_hz, mu_hz_assign)                                                #èµ‹å€¼è¿‡ç¨‹
        
    #================= p(z) =====================
    with tf.variable_scope('p_z'):
        # pi_z_prior[batch_size, cluster_num]
        pi_z_prior = tf.placeholder(dtype=tf.float32, shape=[batch_size, cluster_num], name='pi_z_prior')  #  p(z)=Cat(ğœ‹z)  (6033,200)   è¿™æ˜¯ä¸€ä¸ªè¾“å…¥ é€šè¿‡è®¡ç®—å¯ä»¥å¾—åˆ°ä¸€ä¸ªåˆå§‹å€¼
        _pi_z = tf.get_variable('pi_z', dtype=tf.float32, initializer=tf.ones([batch_size, cluster_num]))
        __pi_z = tf.exp(_pi_z)
        pi_z = tf.div(__pi_z, tf.reduce_sum(__pi_z, -1, keepdims=True))

        pi_z_assign = tf.placeholder(dtype=tf.float32, shape=[batch_size, cluster_num], name='pi_z_assign')  #è¾“å…¥
        initialize_pi_z = tf.assign(_pi_z, pi_z_assign)
    print('Decoders are constructed.')
    
#================= elbo =====================
#ELBOæ•´ä½“å¯ä»¥çœ‹ä½œæ˜¯æ±‚log p(z,y,h,l,x)/q(z,y,h|l,x)  åˆ†å­å¯ä»¥çœ‹ä½œæ˜¯decoderï¼Œåˆ†æ¯ä¸ºencoder
'''
q(h|x) log p(x|h)
q(y|l) log p(l|y)
q(h|x) log q(h|x)
q(y|l) log q(y|l)
q(z|x,l)q(h|x) log p(h|z)
q(z|x,l)q(y|l) log p(y|z)
q(z|x,l) log p(z)
q(z|x,l) log q(z|x,l)
q(z|x,l)
'''
with tf.name_scope('elbo'):
    #================= q(h|x) log p(x|h) =====================
    #q(h|x)  decoderä¸­ è®¡ç®—äº†p(x|h),#å‘é‡ğœŒ_ğ‘¥(h)-->mu_xh  encoderçš„q(h|x)å’Œdecoderçš„p(x|h)
    with tf.name_scope('q_hx_log_p_xh'):
        # reduce_mean along both T and batch_size
        _tmp = tf.reshape(x, [batch_size, 1, feature_size])
        if flag_node_type == 'Bernoulli':
            elbo_q_hx_log_p_xh = tf.reduce_mean(tf.reduce_sum(_tmp*tf.log(1e-10+mu_xh) + (1.0-_tmp)*tf.log(1e-10+(1.0-mu_xh)), -1))
        elif flag_node_type == 'Gaussian':
            elbo_q_hx_log_p_xh = -0.5 * tf.reduce_mean(tf.reduce_sum(tf.square(_tmp-mu_xh), -1))
    
    #================= q(y|l) log p(l|y) =====================
    with tf.name_scope('q_yl_log_p_ly'):
        elbo_q_yl_log_p_ly = -dls.LAA_loss_reconstr(l, pi_ly, pi_yl)      #æ­¤å¤„æ˜¯encoderçš„q(y|l)å’Œdecoderçš„p(l|y)
        
    #================= q(h|x) log q(h|x) =====================     #æ­¤å¤„æ˜¯encoderçš„q(h|x)
    with tf.name_scope('q_hx_log_q_hx'):
        elbo_q_hx_log_q_hx = -0.5 * tf.reduce_mean(tf.reduce_sum(tf.log(1e-10+tf.square(sigma_hx)), -1))

    #================= q(y|l) log q(y|l) =====================    #encoderçš„q(y|l)
    with tf.name_scope('q_yl_log_q_yl'):
        elbo_q_yl_log_q_yl = tf.reduce_mean(tf.reduce_sum(pi_yl * tf.log(1e-10+pi_yl), -1))
    
    #================= q(z|x,l) =====================  è¿™éƒ¨åˆ†è®ºæ–‡è§£é‡Šä¸æ¸…æ¥šï¼Œæš‚æ—¶æ²¡çœ‹æ‡‚
    with tf.name_scope('q_zxl'):
        # p(h|z)[batch_size, T, cluster_num, 1]                #æ­¤éƒ¨åˆ†é€šè¿‡ä»åˆ†å¸ƒq(h|x)çš„Tæ¬¡é‡‡æ ·htæ¥è¿‘ä¼¼
        _h = tf.reshape(embedding_h, [batch_size, T, 1, embedding_size])   #(6033,1,1,40)
        _p_hz = -0.5 * tf.reduce_sum(
            tf.div(tf.square(_h-mu_hz), 1e-10+tf.square(sigma_hz)) 
            + tf.log(1e-10 + tf.square(sigma_hz)), -1, keepdims=True)  #p(h|z) (6033, 1, 200, 1)
        # p_zhy[batch_size, T, cluster_num, category_size]
        _p_zhy = tf.log(1e-10+pi_yz) + _p_hz + tf.log(1e-10+tf.reshape(pi_z, [batch_size, 1, cluster_num, 1]))       #æ­¤éƒ¨åˆ†è®¡ç®—çš„æ˜¯å…¬å¼15ä¸­çš„p(z|y,h) ,å…¶ä¸­p(y = k|z = c)ï¼Œ p(â„^ğ‘¡  |z = c)ï¼Œ p(z = c)åˆ†åˆ«å¯ä»¥ç”±å¯¹åº”çš„åˆ†å¸ƒ(3)ï¼Œ(5)ï¼Œ(2)ç›´æ¥è®¡ç®—
        #ä¸Šé¢çš„ä¸‰é¡¹åˆ†åˆ«ä¸ºp(y|z) p(h|z) p(z)  (6033, 1, 200, 2)
        _p_zhy_max = tf.reduce_max(_p_zhy, 2, keepdims=True)
        #ç±»ä¼¼æœŸæœ›æœ€å¤§åŒ–æ–¹æ³•ä¸­çš„E-stepï¼Œé€šè¿‡æœ€å¤§å€¼ä¼°è®¡æ½œåœ¨å˜é‡åˆ†å¸ƒ
        p_zhy = tf.exp(_p_zhy - (_p_zhy_max + tf.log(1e-10+tf.reduce_sum(tf.exp(_p_zhy-_p_zhy_max), 2, keepdims=True)))) #(6033, 1, 200, 2)
        # q_zxl[batch_size, cluster_num]
        # reduce_mean along both category_size and T
        _q_zxl = tf.reduce_sum(tf.reshape(pi_yl, [batch_size, 1, 1, category_size]) * p_zhy, -1)  #æ±‚å’Œ  (6033, 1, 200)
        q_zxl = tf.reduce_mean(_q_zxl, 1)      #æ±‚å¹³å‡ 1/T   (6033, 200)
        
        # z_index[batch_size]
        z_index = tf.argmax(q_zxl, 1)     #(6033,)
        # cluster_pi_max[batch_size, category_size]
        # cluster_pi_avg[batch_size, category_size]
        cluster_pi_max = tf.gather(pi_yz, z_index)                  #(6033, 2)
        cluster_pi_avg = tf.matmul(q_zxl, pi_yz)                  #(6033, 2)
        
    #================= q(z|x,l)q(h|x) log p(h|z) =====================
    #================= q(h|x) log p(h|z) [batch_size, cluster_num] =====================
    with tf.name_scope('q_zxl_q_hx_log_p_hz'):
        # mu_hx[batch_size, embedding_size]
        # sigma_hx[batch_size, embedding_size]
        # mu_hz[cluster_num, embedding_size]
        # sigma_hz[cluster_num, embedding_size]
        _part_1 = tf.div(tf.square(tf.reshape(mu_hx, [batch_size, 1, embedding_size]) - mu_hz), 1e-10+tf.square(sigma_hz))    #q(h|x)
        _part_2 = tf.div(tf.square(tf.reshape(sigma_hx, [batch_size, 1, -1])), 1e-10+tf.square(sigma_hz))
        _part_3 = tf.log(1e-10 + tf.square(sigma_hz))
        # elbo_q_hx_log_p_hz[batch_size, cluster_num]
        elbo_q_hx_log_p_hz = -0.5 * tf.reduce_sum(_part_1 + _part_2 + _part_3, -1)
        elbo_q_zxl_q_hx_log_p_hz = tf.reduce_mean(tf.reduce_sum(q_zxl * elbo_q_hx_log_p_hz, -1))                                          #VAEå‡½æ•°æŸå¤±ç¬¬ä¸€é¡¹ æœ€å°åŒ–KL
    
    #================= q(z|x,l)q(y|l) log p(y|z) =====================
    #================= q(y|l) log p(y|z) [batch_size, cluster_num] =====================
    with tf.name_scope('q_zxl_q_yl_log_p_yz'):
        # pi_yz[cluster_num, category_size]
        # pi_yl[batch_size, category_size]
        # elbo_q_yl_log_p_yz[batch_size, cluster_num]
        elbo_q_yl_log_p_yz = tf.reduce_sum(tf.reshape(pi_yl, [batch_size, 1, category_size]) * tf.log(1e-10 + pi_yz), -1)
        elbo_q_zxl_q_yl_log_p_yz = tf.reduce_mean(tf.reduce_sum(q_zxl * elbo_q_yl_log_p_yz, -1))
    
    #================= q(z|x,l) log p(z) =====================
    #================= log p(z) [cluster_num] =====================
    with tf.name_scope('q_zxl_log_p_z'):
        # elbo_log_p_z[batch_size, cluster_num]
        elbo_log_p_z = tf.log(1e-10 + pi_z)
        elbo_q_zxl_log_p_z = tf.reduce_mean(tf.reduce_sum(q_zxl * elbo_log_p_z, -1))
    
    #================= q(z|x,l) log q(z|x,l) =====================
    with tf.name_scope('q_zxl_log_q_zxl'):
        # q_zxl[batch_size, cluster_num]
        elbo_q_zxl_log_q_zxl = tf.reduce_mean(tf.reduce_sum(q_zxl * tf.log(1e-10 + q_zxl), -1))
    
    #================= overall elbo ===================== KL
    elbo = elbo_q_hx_log_p_xh + elbo_q_yl_log_p_ly - elbo_q_hx_log_q_hx - elbo_q_yl_log_q_yl         + elbo_q_zxl_q_hx_log_p_hz + elbo_q_zxl_q_yl_log_p_yz + elbo_q_zxl_log_p_z - elbo_q_zxl_log_q_zxl    #KLå…¬å¼å±•å¼€
    
    q_zxl_entropy = -elbo_q_zxl_log_q_zxl
    
    with tf.variable_scope('regularization_prior'):                                                 #æ­£åˆ™åŒ–     mu_hz_prior_mu  è¿™ä¸€é¡¹éœ€è¦å˜åŠ¨æ¢æˆå¯¹æ¯”å­¦ä¹ çš„æ­£åˆ™é¡¹
        mu_hz_prior_mu = tf.placeholder(dtype=tf.float32, shape=[cluster_num, embedding_size], name='mu_hz_prior_mu')
        # sigma_hz_prior_alpha = tf.placeholder(dtype=tf.float32, shape=[cluster_num, embedding_size], name='sigma_hz_prior')
        pi_yz_prior = tf.placeholder(dtype=tf.float32, shape=[cluster_num, category_size], name='pi_yz_prior')
        #çº¦æŸ
        constraint_prior = 0.5*tf.reduce_mean(tf.square(mu_hz - mu_hz_prior_mu))             - tf.reduce_mean(pi_yz_prior * tf.log(1e-10+pi_yz))             - tf.reduce_mean(pi_z_prior * tf.log(1e-10+pi_z))             + tf.reduce_mean(1.0*tf.log(1e-10+tf.square(sigma_hz))+tf.div(2.0, 1e-10+tf.square(sigma_hz)))

    loss_overall = -elbo         + constraint_w_AE         + constraint_w_LAA         + 1.0 * constraint_prior
    
    # optimizier
    learning_rate_overall = 0.001
    optimizer_overall = tf.train.AdamOptimizer(learning_rate=learning_rate_overall).minimize(loss_overall)

    print('Clustering-based label-aware autoencoder is constructed.')

saver = tf.train.Saver()


# In[ ]:


# for epoch in range(1000):
#     # K.clear_session()
#     tf.reset_default_graph()
#     # monitor_mu_hx = 0
#     # monitor_mu_hx1 = 0
#     # x_aug = 0
#     with tf.Session() as sess:
#         sess.run(tf.global_variables_initializer())
#         # _, monitor_loss_square_AE, monitor_mu_hx = sess.run([optimizer_AE, loss_square_AE, mu_hx],
#         #     feed_dict={x:feature})                                                #è¾“å…¥ï¼šç‰¹å¾  è®¡ç®—ç»“æœä¸ºq(h|x)ä¸­çš„h (6033, 40)   å­¦ä¹ ä¸­é—´çš„åµŒå…¥å±‚h-monitor_mu_hx
#         x_aug = x_train + 0.1 * tf.random.normal(tf.shape(x_train), dtype='float32')  # ä½¿ç”¨æ·»åŠ å™ªéŸ³çš„æ–¹å¼æ›¿ä»£VAE ä½œä¸ºå›¾åƒæ•°æ®é›†æ¥è¯´ä¹Ÿä¸æ˜¯ä¸èƒ½è¿™ä¹ˆç”¨
#         print(x_train.shape)
#         print("lblb")
#         inital = model_ema_lb(x_train)
#         # print("lblblbblblbbis",inital)
#         # inital1 = model_ema_lb.predict(x_train)
#         # print("jjjjjjjjjj",inital1)
#         loss, monitor_mu_hx = moco_training_step(x_train, x_aug, queue, model, model_ema, optimizer)
#         #K.clear_session()
#         if epoch % 50 == 0:
#             print("epoch: {0} loss: {1}".format(epoch, loss))
#             #monitor_mu_hx1 = K.eval(monitor_mu_hx)
#             print("lb_over")
#             inital1 = model_ema.predict(x_train)
#             inital2 = model.predict(x_train)
#             print(inital1)
#             print(inital2)
    # result = model_ema_lb.predict(x_train)
    # print(x_train)
    # print("**********lb******")
    # print(result)
    # representation_model = model_ema(inputs=model_ema.inputs, outputs=model_ema.y)
    # dense_3_output = representation_model.predict(x_aug)
    #
    # print(dense_3_output.shape)
    # print(type(dense_3_output))
    # print(dense_3_output)
    # representation_layer = tf.keras.backend.function(inputs=[model_ema.layers[0].input], outputs=[model_ema.layers[1].output])
    # representation_layer = tf.keras.backend.function(inputs=[model_ema.call(x_aug)], outputs=[model_ema.layers[1].output])
    # representation = representation_layer([x_aug])
    # representation = np.array(representation)[0]
    #
    # print(representation.shape)
    # print(type(representation))
    # print(representation)

    #print(monitor_mu_hx1.shape)
# monitor_mu_hx1 = K.eval(monitor_mu_hx)

#================= training and inference =====================
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    #================= pre-train pi_yl =====================
    # assign batch variables (use whole data in one batch)
    # define majority voting regularizer
    majority_y = dls.get_majority_y(user_labels, source_num, category_size)             #è®¡ç®—ä¼°è®¡çš„çœŸå€¼æ ‡ç­¾ç»™å‡ºy


    # pre-train classifier
    #encoder
    print("Pre-train pi_yl ...")   #q(y|l)
    epochs = 50
    for epoch in range(epochs):
        _, monitor_pi_yl = sess.run([optimizer_pre_train_yl, pi_yl], 
            feed_dict={l:user_labels, pi_yl_target:majority_y})                    #è¾“å…¥ï¼šæ ‡ç­¾ä»¥åŠåˆå§‹æŠ•ç¥¨ç»“æœ è®¡ç®—ç»“æœä¸ºyçš„æ¨æ–­æ ‡ç­¾ä¸ºq(y|l)çš„y
        if epoch % 10 == 0:
            hit_num = dls.cal_hit_num(true_labels, monitor_pi_yl)
            print("epoch: {0} accuracy: {1}".format(epoch, float(hit_num)/n_samples))  #æŸ¥çœ‹å‡†ç¡®ç‡
    
    print("Pre-train hx_AE ...")
    epochs = 200
    for epoch in range(epochs):
        _, monitor_loss_square_AE, monitor_mu_hx = sess.run([optimizer_AE, loss_square_AE, mu_hx],
            feed_dict={x:feature})                                                #è¾“å…¥ï¼šç‰¹å¾  è®¡ç®—ç»“æœä¸ºq(h|x)ä¸­çš„h (6033, 40)   å­¦ä¹ ä¸­é—´çš„åµŒå…¥å±‚h-monitor_mu_hx   lb:åŠ å…¥å·¦ä¾§ç»“æœleft result
        # x_aug = x_train + 0.1 * tf.random.normal(tf.shape(x_train), dtype='float32')  # ä½¿ç”¨æ·»åŠ å™ªéŸ³çš„æ–¹å¼æ›¿ä»£VAE ä½œä¸ºå›¾åƒæ•°æ®é›†æ¥è¯´ä¹Ÿä¸æ˜¯ä¸èƒ½è¿™ä¹ˆç”¨
        # inital = model_ema_lb(x_train)
        # loss, monitor_mu_hx = moco_training_step(x_train, x_aug, queue, model, model_ema, optimizer)
        if epoch % 50 == 0:
            print("epoch: {0} loss: {1}".format(epoch, monitor_loss_square_AE))
            # K.clear_session()
            #print(monitor_mu_hx)
            # print(monitor_mu_hx[0])
            # print(majority_y[0])
            # print(np.shape(np.concatenate((monitor_mu_hx, majority_y), 1)))
            # print((np.concatenate((monitor_mu_hx, majority_y), 1))[0])
            # clustering_result = KMeans(n_clusters=cluster_num).fit(np.concatenate((monitor_mu_hx, majority_y), 1))
            # print(clustering_result.labels_)
            # print(np.shape(clustering_result.labels_))
    # monitor_mu_hx1 = K.eval(monitor_mu_hx)
    # monitor_mu_hx1 = model_ema.predict(x_train)
    # print(monitor_mu_hx1)
            #print(monitor_mu_hx1.shape)
            #================= calculate initial parametersè®¡ç®—åˆå§‹å‚æ•° =====================æ­¤éƒ¨åˆ†éœ€è¦è¿›è¡Œä¸Šé¢ä¸€äº›ä¸è®­ç»ƒçš„æ“ä½œ
        clustering_result = KMeans(n_clusters=cluster_num).fit(np.concatenate((monitor_mu_hx, majority_y), 1))    #æ‹¼æ¥,èšç±» cluster_num=200     èšç±»éƒ¨åˆ†èƒ½å¦è€ƒè™‘ä¼˜åŒ–ï¼Ÿ (6033,42)
    print("kmeans over")
    labels_lb = clustering_result.labels_
    tsne = TSNE(n_components=2,random_state=0)
    X_tsne = tsne.fit_transform(np.concatenate((monitor_mu_hx, majority_y), 1))

    plt.figure(figsize=(8, 6))
    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels_lb, cmap='rainbow')
    plt.title("K-Means Clustering with t-SNE Visualization")
    plt.xlabel("t-SNE Dimension 1")
    plt.ylabel("t-SNE Dimension 2")
    plt.colorbar()
    plt.show()

    # å¯è§†åŒ–å‰20ä¸ªç°‡çš„é™ç»´ç»“æœ
    # plt.figure(figsize=(8, 6))
    # for cluster_id in range(20):
    #     cluster_points = X_tsne[labels == cluster_id]
    #     plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster_id}')
    #
    # plt.title("K-Means Clustering with t-SNE Visualization")
    # plt.xlabel("t-SNE Dimension 1")
    # plt.ylabel("t-SNE Dimension 2")
    # plt.legend()
    # plt.show()

    # pi_z_prior_cluster = np.ones([n_samples, cluster_num]) / cluster_num
    pi_z_prior_cluster = dls.convert_to_one_hot(clustering_result.labels_, cluster_num, smooth=0.2)       #è¾“å…¥  (6033, 200) cluster_num=200    ç¼–ç ä¸ºonehotå‘é‡   clustering_result.labels_ :èšç±»åçš„åˆ†ç±»æƒ…å†µ200ä¸ªç°‡é‡Œé¢ p(ğœ‹z)
    _ = sess.run(initialize_mu_hz, {mu_hz_assign:clustering_result.cluster_centers_[:, 0:embedding_size]})        #è®¡ç®—ğœ‡hz,p(ğœ‡hz)=N(ğœ‡2,I)
    # pi_yz_prior_cluster = np.ones([cluster_num, category_size]) / cluster_num
    pi_yz_prior_cluster = dls.get_cluster_majority_y(
        clustering_result.labels_, user_labels, cluster_num, source_num, category_size)       #é€šè¿‡æ”¶é›†é›†ç¾¤ä¸­çš„ç”¨æˆ·æ ‡ç­¾ï¼Œä¸ºæ¯ä¸ªé›†ç¾¤è¿”å›y  (200, 2) ä¸€å…±200ä¸ªç°‡  ç°‡æ ‡ç­¾
    _ = sess.run(initialize_pi_yz, {pi_yz_assign:pi_yz_prior_cluster})                          #åˆå§‹åŒ–p(ğœ‹yz)=Dir(ğ›½^ğ‘§)
    _ = sess.run(initialize_pi_z, {pi_z_assign:pi_z_prior_cluster})                             #åˆå§‹åŒ–p(ğœ‹z)=Dir(ğ›¼^ğ‘š)

    mu_hz_prior_mu_cluster = clustering_result.cluster_centers_[:, 0:embedding_size]         #è¾“å…¥   ç°‡ä¸­å¿ƒ

    predict_label = np.zeros([batch_size, category_size])     #(6033,2)   æœ€ç»ˆçš„åˆå§‹åŒ–æ ‡ç­¾
    for i in range(batch_size):
        predict_label[i] = pi_yz_prior_cluster[clustering_result.labels_[i], :]
    print("Initial clustering accuracy: {0}".format(float(dls.cal_hit_num(true_labels, predict_label)) / n_samples))       #åˆå§‹åŒ–æ—¶çš„å‡†ç¡®ç‡

    #================= save current model =====================
    saved_path = saver.save(sess, './my_model')


# In[ ]:
# K.clear_session()

with tf.Session() as sess:
    saver.restore(sess, './my_model')

    print("Train overall net ...")
    epochs = 5000
    for epoch in range(epochs):
        _, monitor_loss_overall, monitor_pi_yl, monitor_cluster_pi_max, monitor_cluster_pi_avg,             monitor_constraint_w_AE, monitor_constraint_prior = sess.run(
                [optimizer_overall, loss_overall, pi_yl, cluster_pi_max, cluster_pi_avg, constraint_w_AE, constraint_prior],
                feed_dict={l:user_labels, x:feature,
                           pi_z_prior:pi_z_prior_cluster,
                           mu_hz_prior_mu:mu_hz_prior_mu_cluster,
                           pi_yz_prior:pi_yz_prior_cluster})                  #håä¸‰é¡¹è®¡ç®—lossä½¿ç”¨
        if epoch % 10 == 0:
            print("epoch: {0} loss: {1}".format(epoch, monitor_loss_overall))
            print("epoch: {0} loss: {1}".format(epoch, monitor_constraint_w_AE))
            print("epoch: {0} loss: {1}".format(epoch, monitor_constraint_prior))
            hit_num_cluster_level_avg = dls.cal_hit_num(true_labels, monitor_cluster_pi_avg)
            print("epoch: {0} accuracy(cluster level avg): {1}".format(epoch, float(hit_num_cluster_level_avg)/n_samples))
    print("Training overall net. Done!")
